{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Nonlinear inversion with box constraints\n",
    "In this tutorial we focus on a modification of the `Quadratic program\n",
    "with box constraints` tutorial where the quadratic function is replaced by a\n",
    "nonlinear function. For this example we will use the well-known Rosenbrock\n",
    "function:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x} = \\argmin_\\mathbf{x} f(\\mathbf{x}) \\quad \\text{s.t.} \\quad \\mathbf{x}\n",
    "        \\in \\mathcal{I}_{\\operatorname{Box}}\n",
    "\n",
    "We will learn how to handle nonlinear functionals in convex optimization, and\n",
    "more specifically dive into the details of the\n",
    ":class:`pyproximal.proximal.Nonlinear` operator. This is a template operator\n",
    "which must be subclassed and used for a specific functional. After doing so, we will\n",
    "need to implement the following three method: `func` and `grad` and `optimize`.\n",
    "As the names imply, the first method takes a model vector $x$ as input and\n",
    "evaluates the functional. The second method evaluates the gradient of the\n",
    "functional with respect to $x$. The third method implements an\n",
    "optimization routine that solves the proximal operator of $f$,\n",
    "more specifically:\n",
    "\n",
    "    .. math::\n",
    "        \\prox_{\\tau f} (\\mathbf{x}) = \\argmin_{\\mathbf{y}} f(\\mathbf{y}) +\n",
    "        \\frac{1}{2 \\tau}\\|\\mathbf{y} - \\mathbf{x}\\|^2_2\n",
    "\n",
    "Note that when creating the ``optimize`` method a user must use the gradient\n",
    "of the augmented functional which is provided by the `_gradprox` built-in\n",
    "method in :class:`pyproximal.proximal.Nonlinear` class.\n",
    "\n",
    "In this example, we will consider both the\n",
    ":func:`pyproximal.optimization.primal.ProximalGradient` and\n",
    ":func:`pyproximal.optimization.primal.ADMM` algorithms. The former solver\n",
    "will simply use the `grad` method whilst the second solver relies on the\n",
    "`optimize` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyproximal\n",
    "\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the class for the nonlinear functional\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y, a=1, b=10):\n",
    "    f = (a - x)**2 + b*(y - x**2)**2\n",
    "    return f\n",
    "\n",
    "def rosenbrock_grad(x, y, a=1, b=10):\n",
    "    dfx = -2*(a - x) - 2*b*(y - x**2) * 2 * x\n",
    "    dfy = 2*b*(y - x**2)\n",
    "    return dfx, dfy\n",
    "\n",
    "def contour_rosenbrock(x, y):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Evaluate the function\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    z = rosenbrock(x, y)\n",
    "\n",
    "    # Plot the surface.\n",
    "    surf = ax.contour(x, y, z, 200, cmap='gist_heat_r', vmin=-20, vmax=200,\n",
    "                      antialiased=False)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "    return fig, ax\n",
    "\n",
    "class Rosebrock(pyproximal.proximal.Nonlinear):\n",
    "    def setup(self, a=1, b=10, alpha=1.):\n",
    "        self.a, self.b = a, b\n",
    "        self.alpha = alpha\n",
    "    def fun(self, x):\n",
    "        return np.array(rosenbrock(x[0], x[1], a=self.a, b=self.b))\n",
    "    def grad(self, x):\n",
    "        return np.array(rosenbrock_grad(x[0], x[1], a=self.a, b=self.b))\n",
    "    def optimize(self):\n",
    "        self.solhist = []\n",
    "        sol = self.x0.copy()\n",
    "        for iiter in range(self.niter):\n",
    "            x1, x2 = sol\n",
    "            dfx1, dfx2 = self._gradprox(sol, self.tau)\n",
    "            x1 -= self.alpha * dfx1\n",
    "            x2 -= self.alpha * dfx2\n",
    "            sol = np.array([x1, x2])\n",
    "            self.solhist.append(sol)\n",
    "        self.solhist = np.array(self.solhist)\n",
    "        return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now setup the problem and solve it without constraints using a simple\n",
    "gradient descent with fixed-step size (of course we could choose any other\n",
    "solver)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 500\n",
    "alpha = 0.02\n",
    "\n",
    "steps = [(0, 0), ]\n",
    "for iiter in range(niters):\n",
    "    x, y = steps[-1]\n",
    "    dfx, dfy = rosenbrock_grad(x, y)\n",
    "    x -= alpha * dfx\n",
    "    y -= alpha * dfy\n",
    "    steps.append((x, y))\n",
    "\n",
    "x = np.arange(-1.5, 1.5, 0.15)\n",
    "y = np.arange(-0.5, 1.5, 0.15)\n",
    "nx, ny = len(x), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the box constraint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbound = np.arange(-1.5, 1.5, 0.01)\n",
    "ybound = np.arange(-0.5, 1.5, 0.01)\n",
    "X, Y = np.meshgrid(xbound, ybound, indexing='ij')\n",
    "xygrid = np.vstack((X.ravel(), Y.ravel()))\n",
    "\n",
    "lower = 0.6\n",
    "upper = 1.2\n",
    "indic = (xygrid > lower) & (xygrid < upper)\n",
    "indic = indic[0].reshape(xbound.size, ybound.size) & \\\n",
    "        indic[1].reshape(xbound.size, ybound.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now solve the constrained optimization using the Proximal gradient solver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnl = Rosebrock(niter=20, x0=np.zeros(2), warm=True)\n",
    "fnl.setup(1, 10, alpha=0.02)\n",
    "ind = pyproximal.proximal.Box(lower, upper)\n",
    "\n",
    "def callback(x):\n",
    "    xhist.append(x)\n",
    "\n",
    "x0 = np.array([0, 0])\n",
    "xhist = [x0,]\n",
    "xinv_pg = pyproximal.optimization.primal.ProximalGradient(fnl, ind,\n",
    "                                                          tau=0.001,\n",
    "                                                          x0=x0, epsg=1.,\n",
    "                                                          niter=5000, show=True,\n",
    "                                                          callback=callback)\n",
    "xhist_pg = np.array(xhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using the ADMM solver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0, 0])\n",
    "\n",
    "xhist = [x0,]\n",
    "xinv_admm = pyproximal.optimization.primal.ADMM(fnl, ind,\n",
    "                                                tau=1.,\n",
    "                                                x0=x0,\n",
    "                                                niter=30, show=True,\n",
    "                                                callback=callback)\n",
    "xhist_admm = np.array(xhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude it is important to notice that whilst we implemented a vanilla\n",
    "gradient descent inside the optimize method, any more advanced solver can\n",
    "be used (here for example we will repeat the same exercise using L-BFGS from\n",
    "scipy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rosebrock_lbfgs(Rosebrock):\n",
    "    def optimize(self):\n",
    "        def callback(x):\n",
    "            self.solhist.append(x)\n",
    "\n",
    "        self.solhist = []\n",
    "        self.solhist.append(self.x0)\n",
    "        sol = sp.optimize.minimize(lambda x: self._funprox(x, self.tau),\n",
    "                                   x0=self.x0,\n",
    "                                   jac=lambda x: self._gradprox(x, self.tau),\n",
    "                                   method='L-BFGS-B', callback=callback,\n",
    "                                   options=dict(maxiter=15))\n",
    "        sol = sol.x\n",
    "\n",
    "        self.solhist = np.array(self.solhist)\n",
    "        return sol\n",
    "\n",
    "\n",
    "fnl = Rosebrock_lbfgs(niter=20, x0=np.zeros(2), warm=True)\n",
    "fnl.setup(1, 10, alpha=0.02)\n",
    "\n",
    "x0 = np.array([0, 0])\n",
    "xhist = [x0,]\n",
    "xinv_admm_lbfgs = pyproximal.optimization.primal.ADMM(fnl, ind,\n",
    "                                                      tau=1.,\n",
    "                                                      x0=x0,\n",
    "                                                      niter=30, show=True,\n",
    "                                                      callback=callback)\n",
    "xhist_admm_lbfgs = np.array(xhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's compare the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = contour_rosenbrock(x, y)\n",
    "steps = np.array(steps)\n",
    "ax.plot(steps[:, 0], steps[:, 1], '.-k', lw=2, ms=20, alpha=0.4)\n",
    "ax.contour(X, Y, indic, colors='k')\n",
    "ax.scatter(1, 1, c='k', s=300)\n",
    "ax.plot(xhist_pg[:, 0], xhist_pg[:, 1], '.-b', ms=20, lw=2, label='PG')\n",
    "ax.plot(xhist_admm[:, 0], xhist_admm[:, 1], '.-g', ms=20, lw=2, label='ADMM')\n",
    "ax.plot(xhist_admm_lbfgs[:, 0], xhist_admm_lbfgs[:, 1], '.-m', ms=20, lw=2,\n",
    "        label='ADMM with LBFGS')\n",
    "ax.set_title('Rosenbrock optimization')\n",
    "ax.legend()\n",
    "ax.set_xlim(x[0], x[-1])\n",
    "ax.set_ylim(y[0], y[-1])\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
